- activation


![alt text](image-3.png)
![alt text](image-4.png)
![alt text](image-5.png)
![alt text](image-23.png)
- loss


![alt text](image-6.png)
![alt text](image-7.png)
![alt text](image-8.png)
![alt text](image-9.png)

![alt text](image-10.png)
![alt text](image-11.png)
![alt text](image-12.png)
![alt text](image-13.png)
![alt text](image-14.png)
![alt text](image-15.png)
![alt text](image-16.png)
- optimization


    - convex
    - is mse convex loss
    - gradient descent
    ![alt text](image.png)
    ![alt text](image-17.png)
    ![alt text](image-18.png)
    ![alt text](image-19.png)
    ![alt text](image-20.png)
    ![alt text](image-22.png)
    ![alt text](image-26.png)
    ![alt text](image-27.png)
    ![alt text](image-28.png)
    ![alt text](image-29.png)
    ![alt text](image-30.png)
    ![alt text](image-31.png)
    ![alt text](image-32.png)
- vanishing and exploding gradients


![alt text](image-33.png)
![alt text](image-34.png)
- backprop and lr


![alt text](image-1.png)
![alt text](image-2.png)


- dropout


![alt text](image-35.png)
- batch norm


![alt text](image-21.png)
![alt text](image-36.png)

- layer norm 


![alt text](image-38.png)

- rnn, lstm, gru, transformer
- auto encoder

- how did you choose the layers, units, act functions and why
- preprocess ts how did you do it
- nhits nbeats
- weights


![alt text](image-24.png)
![alt text](image-25.png)
![alt text](image-39.png)
![alt text](image-40.png)


