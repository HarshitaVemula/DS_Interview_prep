1. prob cheat sheet
    imp concepts

    
    ![alt text](image-43.png)

    - Kurtosis - measures shape of distribution - especially the tails, are thery heavire or lighter than normal. normal dist has kurtosis = 0

    - var 
        population has var which indicates avg spread
        var = (x-mu)^2/n = sigma^2

        var of estimator measures the variability of an estimator across different samples from the population.
        var = E(Xhat^2) - (E(Xhat))^2 = E((Xhat-E(Xhat))^2) 

        
    - bias = X- E(Xhat)
        In general, a population does not have bias, because the population represents the entire set of data or observations, encompassing all possible values for a parameter. Bias typically arises when working with samples, estimators, or models, which are imperfect representations of the population.

        The bias quantifies how far the expected value of the estimator is from the true population parameter.

        ![alt text](image-27.png)

        ![alt text](image-28.png)

    - LLN - as n tends to inifity, sample mean will tend to pop mean
    assumption - x are iid rv with constant mean mu

    - CLT - sum of iid rvs will tend to normal as n tends to infinity
    It states that, given a sufficiently large sample size from a population with any distribution, the sampling distribution of the sample mean will approximate a normal distribution, regardless of the original population's distribution.

    ![alt text](image-10.png)

    - distributions

    ![alt text](image-11.png)

2. sample statistics

    ![alt text](image-7.png)

    - Statistic is an estimate of the parameter.
    - sample bias

    ![sample bias](image-8.png)

    - sample error

    ![alt text](image-6.png)

    - sampling variability

    ![var](image-9.png)

    - standard err and stand dist diff

    ![alt text](image-14.png) | ![alt text](image-17.png)

    ![alt text](image-16.png)

    - intervals
    ![alt text](image-19.png)

    ![alt text](image-26.png)

    ![alt text](image-21.png)

    ![alt text](image-22.png)

    ![alt text](image-25.png)

3. Probability

    - odds

    ![alt text](image-12.png)

    - prior, posterior, likelihood, marginal

    ![alt text](image-13.png)

4. Hyp tests

![alt text](image-29.png)

- explain p val to non tech audience

![alt text](image-30.png)

![alt text](image-31.png)



- errs

![alt text](image-32.png)

![alt text](image-33.png)

- power

![alt text](image-44.png)

![alt text](image-34.png)

![alt text](image-36.png)

![alt text](image-37.png)

![alt text](image-35.png)

![alt text](image-45.png)

![alt text](image-47.png)

![alt text](image-48.png)

![alt text](image-49.png)

- Hyp testing
![alt text](image-38.png)

![alt text](image-39.png)

![alt text](image-40.png)

![alt text](image-41.png)

    - t dist has fatter tails compared to normal

![alt text](image-42.png)

![alt text](image-46.png)

![alt text](image-50.png)

![alt text](image-51.png)

![alt text](image-52.png)

![alt text](image-53.png)

![alt text](image-62.png)

![alt text](image-54.png)

![alt text](image-55.png)
![alt text](image-56.png)

![alt text](image-57.png)
![alt text](image-58.png)

![alt text](image-59.png)
![alt text](image-60.png)


- pitfalls of experimentation 





- sampling and sampling bias

- metrics

![alt text](image-61.png)

    - r^2, adj r^2
    
    ![alt text](image-63.png)

    - f stat
    ![alt text](image-64.png)

    - auc roc and pre recall

    ![alt text](image-65.png)

    - entropy
    - gini
    - silhouette score

- loss
    - hinge loss
    - log loss
    - mse
    - clustering loss
    - 


- causal inference










